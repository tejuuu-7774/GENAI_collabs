{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0Ks4M06y3jtpoOl5IMBMF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tejuuu-7774/GENAI_collabs/blob/main/RNN_MODEL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "908UPN-QtBXU"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "# from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import SimpleRNN , Dense , Embedding\n",
        "# from tensorflow.keras.datasets import imdb\n",
        "# from tensorflow.keras import models\n",
        "# import tensorflow.keras as keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (xtrain , ytrain) , (xtest, ytest) = imdb.load_data()"
      ],
      "metadata": {
        "id": "v0CjtdBW0bNh"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xtrain"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JtkjDmoG0bLf"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # CORPUS -> List of distinct words\n",
        "# len(imdb.get_word_index())"
      ],
      "metadata": {
        "id": "qsMv6biF0bI4"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # View the words\n",
        "# imdb.get_word_index()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GUkWNkMv0bAj"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # TO DECODE THE ABOVE VECTOR NUMBERS TO WORD\n",
        "# word_index = imdb.get_word_index()\n",
        "# reverse_word_index = {value: key for key, value in word_index.items()}\n",
        "# print(reverse_word_index.get(1))\n",
        "# # first three characters are reserved for special characters so we start from 4-3 or 1  to check the first word"
      ],
      "metadata": {
        "id": "b63R1-vA3Ng5"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xtrainP = pad_sequences(xtrain, padding='pre',maxlen=2494)\n",
        "# xtrainP.shape"
      ],
      "metadata": {
        "id": "da3veDMe3Zrr"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# xtestP = pad_sequences(xtest, padding='pre',maxlen=2494)\n",
        "# xtestP.shape"
      ],
      "metadata": {
        "id": "EuAMC3fR3sj_"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Model architecture\n",
        "# imdb_model = models.Sequential()"
      ],
      "metadata": {
        "id": "-0uJa9UX3sg6"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Embedding\n",
        "# imdb_model.add(\n",
        "#     keras.layers.Embedding(\n",
        "#         input_dim=88584,\n",
        "#         output_dim=15,\n",
        "#         input_length=2494\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "6Zhy0_uG3sdR"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Dense neural network\n",
        "# imdb_model.add(keras.layers.Flatten())\n",
        "# imdb_model.add(keras.layers.Dense(8))\n",
        "# imdb_model.add(keras.layers.Dense(1,activation='sigmoid'))"
      ],
      "metadata": {
        "id": "r6q_Arsk5lTp"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# FLOW CHART\n",
        "# INPUT DATA -> EMBEDDING (2494 , 15)-> FLATTEN (to convert to 1dimen) -> Hidden layer -> Output layer"
      ],
      "metadata": {
        "id": "CxQ-9PWy6amj"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imdb_model.compile(\n",
        "#     optimizer='adam',\n",
        "#     loss='binary_crossentropy',\n",
        "#     metrics=['accuracy']\n",
        "# )"
      ],
      "metadata": {
        "id": "_4i0oWjp6aeN"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imdb_model.fit(\n",
        "#   xtrainP,\n",
        "#   ytrain,\n",
        "#   epochs=10,\n",
        "#   validation_data=(xtestP, ytest)\n",
        "# )"
      ],
      "metadata": {
        "id": "rRbZldZY8NmB"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(imdb_model.predict(xtestP[56].reshape(1,2494)))"
      ],
      "metadata": {
        "id": "IzhWQhtO8NAS"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import numpy as np\n",
        "\n",
        "# # 1. Extract the 15D map from the Embedding layer\n",
        "# embedding_matrix = imdb_model.layers[0].get_weights()[0]\n",
        "\n",
        "# # 2. Build a reverse dictionary (ID -> Word)\n",
        "# word_index = imdb.get_word_index()\n",
        "# reverse_word_index = {value + 3: key for key, value in word_index.items()}\n",
        "# reverse_word_index[0] = \"<PAD>\"\n",
        "# reverse_word_index[1] = \"<START>\"\n",
        "# reverse_word_index[2] = \"<UNKNOWN>\"\n",
        "\n",
        "# def find_closest_words(target_word, top_n=5):\n",
        "#     # 3. Find the ID of the word you typed\n",
        "#     if target_word not in word_index:\n",
        "#         return f\"Sorry, '{target_word}' is not in the dictionary.\"\n",
        "\n",
        "#     target_id = word_index[target_word] + 3\n",
        "\n",
        "#     # 4. Grab the 15 mathematical coordinates for that word\n",
        "#     target_vector = embedding_matrix[target_id]\n",
        "\n",
        "#     # 5. Calculate the distance between this word and ALL 88,586 other words\n",
        "#     distances = np.linalg.norm(embedding_matrix - target_vector, axis=1)\n",
        "\n",
        "#     # 6. Sort the distances to find the closest ones\n",
        "#     closest_ids = distances.argsort()[1:top_n+1]\n",
        "\n",
        "#     # 7. Print the results\n",
        "#     print(f\"\\nWords closest to '{target_word}' in the 15D AI Map:\")\n",
        "#     print(\"-\" * 40)\n",
        "#     for i, word_id in enumerate(closest_ids):\n",
        "#         word = reverse_word_index.get(word_id, \"<UNKNOWN>\")\n",
        "#         dist = distances[word_id]\n",
        "#         print(f\"{i+1}. {word} (Distance: {dist:.4f})\")\n",
        "\n",
        "# # --- Let's test it out! ---\n",
        "# find_closest_words(\"excellent\")\n",
        "# find_closest_words(\"terrible\")\n",
        "# find_closest_words(\"boring\")"
      ],
      "metadata": {
        "id": "4ojSLk3y9S7o"
      },
      "execution_count": 90,
      "outputs": []
    }
  ]
}