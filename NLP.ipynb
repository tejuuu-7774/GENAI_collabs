{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IC6um1CghXeS",
        "Rcmy4zyQiN6C",
        "2hO1wVP9ln5l",
        "4JsqEMAMnlBK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#09-02 ___ PROCESSING OUR LANGUAGE BEFORE FEEDING INTO OUR MODELS."
      ],
      "metadata": {
        "id": "Fob4q9Lzs664"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural language processing ( NLP )\n"
      ],
      "metadata": {
        "id": "IC6um1CghXeS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "uynLh7mohNzX"
      },
      "outputs": [],
      "source": [
        "# import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# text = \"Ugh... The deliveries were DELAYED! I seriously hate waiting... #annoyed\""
      ],
      "metadata": {
        "id": "psAeaoYehdMJ"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert all letters to either in lower case or upper case but all should be converted.\n",
        "# text_lower = text.lower()\n",
        "# text_lower"
      ],
      "metadata": {
        "id": "43nUl5LJhmrK"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove punctuations using string translation\n",
        "# translator = str.maketrans('','',string.punctuation)\n",
        "# clean_text = text_lower.translate(translator)"
      ],
      "metadata": {
        "id": "dw47J8erhmo1"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Original: \",text);\n",
        "# print(\"Cleaned: \",clean_text)"
      ],
      "metadata": {
        "id": "HBL9FiqmhmmP"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n"
      ],
      "metadata": {
        "id": "Rcmy4zyQiN6C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We cannot read whole sentence at once we read word by word so to split them.\n",
        "# It is the foundational process of breaking down raw text into smaller, meaningful units called tokens, such as words, sub-words, or characters."
      ],
      "metadata": {
        "id": "vZ6vs4UsiQZN"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD A:\n",
        "# tokens = clean_text.split()"
      ],
      "metadata": {
        "id": "vKlUGcqxi6-H"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Tokens: \",tokens);\n",
        "# print(\"Token Count: \",len(tokens))"
      ],
      "metadata": {
        "id": "JbOugoCZinxv"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We cannot read \"microtransactional\" in one go like we cannot read a sentence so the main sophisticated way to let computer understand our language also."
      ],
      "metadata": {
        "id": "9rkab3IKjGzo"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD B:\n",
        "# from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "XOeyDBnri5OL"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use BERT tokenizer ( Standard in industry )\n",
        "# tokensizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# complex_text = \"The microtransactional system was counterintuitive.\""
      ],
      "metadata": {
        "id": "QKGaHvfEjWet"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"Tokens: \",tokensizer.tokenize(complex_text))"
      ],
      "metadata": {
        "id": "TVyTqmihjWTN"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets try with a simple sentence now!\n",
        "# tokensizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "# print(\"Tokens: \",tokensizer.tokenize(clean_text))"
      ],
      "metadata": {
        "id": "rwbQRkptj9ic"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It cannot understand our language! so we use the installed version for it.\n",
        "# It costs us a lot to use a tokenizer so many countries are struggling for it.\n",
        "# !pip install indic-nlp-library\n",
        "# from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "# text = 'मैं आज खुश हूँ'\n",
        "# tokens = indic_tokenize.trivial_tokenize(text)\n",
        "# print(tokens)"
      ],
      "metadata": {
        "id": "mCNk5jqMkx3t"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOP WORD REMOVAL\n"
      ],
      "metadata": {
        "id": "2hO1wVP9ln5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # THE , IS , I , AM --> These are basically words which mean soo much but only used for a structured sentence.\n",
        "# import nltk\n",
        "# import string\n",
        "# from nltk.corpus import stopwords\n",
        "# nltk.download('stopwords')\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# # METHOD A\n",
        "# text = \"Ugh... The deliveries were DELAYED! I seriously hate waiting... #annoyed\"\n",
        "# text_lower = text.lower()\n",
        "# translator = str.maketrans('','',string.punctuation)\n",
        "# clean_text = text_lower.translate(translator)\n",
        "# tokens = clean_text.split()\n",
        "\n",
        "# filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "# print(\"Input: \",tokens)\n",
        "# print(\"Result: \",filtered_tokens)"
      ],
      "metadata": {
        "id": "DYwYS6L8lqtZ"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEMMING VS. LEMMATIZATION.\n"
      ],
      "metadata": {
        "id": "4JsqEMAMnlBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming -> used to chop words to convert into root form but it might result in non-words\n",
        "# Lemmatization -> searches for a dictionary root form and add back meaning to our word.\n",
        "# import nltk\n",
        "# from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# nltk.download('wordnet')\n",
        "# stemmer = PorterStemmer()\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# words = [\"deliveries\",\"waiting\",\"delayed\",\"studies\"]\n",
        "\n",
        "# for w in words:\n",
        "#   print(w,\"|\",stemmer.stem(w),\"|\",lemmatizer.lemmatize(w))"
      ],
      "metadata": {
        "id": "ZwzsW6OunpTt"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VECTORIZATION ( TEXT TO NUMBERS )\n"
      ],
      "metadata": {
        "id": "bvK335p3pTzE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This way of numbering text introduces bias -> so to prevent label encoding we use one-hot encoding but soon\n",
        "# if we realise in one-hot encoding it creates multiple columns which takes up lots of space.\n",
        "\n",
        "# HERE IT COMES -> BAG OF WORDS\n",
        "# All the sentences having similar words carry similar meaning\n",
        "# BIG PROBLEMS OF BAG OF WORDS\n",
        "# 1. FOR THIS SYSTEM : \"DOG BITES MAN\" WILL BE SAME AS \"MAN BITES DOG\" but in reality it is different."
      ],
      "metadata": {
        "id": "DCuJby0BpXss"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTOR EMBEDDINGS:\n",
        "# !pip install sentence_transformers\n",
        "# from sentence_transformers import SentenceTransformer\n",
        "# model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# # 2. Give it words\n",
        "# words = [\"She is courageous\",'She acts strong']\n",
        "# embeddings = model.encode(words)\n",
        "\n",
        "# # 3. see the vectors\n",
        "# for i in range(2):\n",
        "#   print(\"Word: \",words[i])\n",
        "#   print(\"Vector: \",embeddings[i][:3])"
      ],
      "metadata": {
        "id": "RY16Vv3VrPvY"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COS_SIM IS USED TO TELL HOW SIMILAR OUR WORDS ARE.\n",
        "# from sentence_transformers import util\n",
        "# util.cos_sim(embeddings[0],embeddings[1])"
      ],
      "metadata": {
        "id": "Yog8_LeerPm4"
      },
      "execution_count": 97,
      "outputs": []
    }
  ]
}