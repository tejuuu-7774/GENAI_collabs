{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# NLP lab - 9/02\n"
      ],
      "metadata": {
        "id": "kWsuyOXblmoP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "YM_ORetlhHs2"
      },
      "outputs": [],
      "source": [
        "# # If our data consists of too many zeroes its known as sparse data.\n",
        "# # NLTK -> NATURAL LANGUAGE TOOL KIT.\n",
        "# import pandas as pd\n",
        "# import string\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('omw-1.4')\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Create DataFrame\n",
        "# df = pd.DataFrame({\n",
        "#   \"raw_text\": [\n",
        "#   \"The food was amazing, and the tea was hot!!!\",\n",
        "#   \"Bad service... I waited 20 minutes for a sandwich?\",\n",
        "#   \"The samosas are tasty but the coffee is too sweet.\",\n",
        "#   \"Cleaning was not good, very dirty tables.\"\n",
        "#   ]\n",
        "# })\n",
        "# df.head()"
      ],
      "metadata": {
        "id": "J9khPqKRl__7"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 1. LOWERCASE\n",
        "# df['processed'] = df['raw_text'].str.lower();\n",
        "# df"
      ],
      "metadata": {
        "id": "V-dktQY1nteX"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 2. Remove punctuations in the sentences.\n",
        "# df['processed'] = df['processed'].apply(lambda x:x.translate(str.maketrans('','',string.punctuation)))\n",
        "# df"
      ],
      "metadata": {
        "id": "teG0_5icntM-"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Tokenization\n",
        "# df['tokens'] = df['processed'].apply(word_tokenize)\n",
        "# df['tokens']"
      ],
      "metadata": {
        "id": "0J-uVCqnpsfR"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # stop-word removal\n",
        "# stop_words = set(stopwords.words('english'))\n",
        "# df['no_stops'] = df['tokens'].apply(lambda x: [word for word in x if word not in stop_words])\n",
        "# df"
      ],
      "metadata": {
        "id": "Mn7bJwiZpscU"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Lemmatization -> meaningful words\n",
        "# df['lemmatized'] = df['no_stops'].apply(lambda x: [WordNetLemmatizer().lemmatize(word) for word in x])\n",
        "# df"
      ],
      "metadata": {
        "id": "ECEkNgPepsUF"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# cv = CountVectorizer(analyzer=lambda x: x)\n",
        "# bow_matrix = cv.fit_transform(df['lemmatized'])\n",
        "# df_bow = pd.DataFrame(bow_matrix.toarray(), columns=cv.get_feature_names_out())\n",
        "# df_bow\n",
        "\n",
        "\n",
        "# # BAG OF WORDS -> TEXT TO NUMBERS THERE EXISTS MORE ALGORITHMS."
      ],
      "metadata": {
        "id": "SQ3-lBgmtQeK"
      },
      "execution_count": 59,
      "outputs": []
    }
  ]
}