{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "IC6um1CghXeS",
        "Rcmy4zyQiN6C",
        "2hO1wVP9ln5l",
        "4JsqEMAMnlBK"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#09-02 ___ PROCESSING OUR LANGUAGE BEFORE FEEDING INTO OUR MODELS."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural language processing ( NLP )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Ugh... The deliveries were DELAYED! I seriously hate waiting... #annoyed\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Convert all letters to either in lower case or upper case but all should be converted.\n",
        "text_lower = text.lower()\n",
        "text_lower"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Remove punctuations using string translation\n",
        "translator = str.maketrans('','',string.punctuation)\n",
        "clean_text = text_lower.translate(translator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Original: \",text);\n",
        "print(\"Cleaned: \",clean_text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We cannot read whole sentence at once we read word by word so to split them.\n",
        "# It is the foundational process of breaking down raw text into smaller, meaningful units called tokens, such as words, sub-words, or characters."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD A:\n",
        "tokens = clean_text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens: \",tokens);\n",
        "print(\"Token Count: \",len(tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We cannot read \"microtransactional\" in one go like we cannot read a sentence so the main sophisticated way to let computer understand our language also."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# METHOD B:\n",
        "from transformers import AutoTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We use BERT tokenizer ( Standard in industry )\n",
        "tokensizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "complex_text = \"The microtransactional system was counterintuitive.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Tokens: \",tokensizer.tokenize(complex_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets try with a simple sentence now!\n",
        "tokensizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "print(\"Tokens: \",tokensizer.tokenize(clean_text))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# It cannot understand our language! so we use the installed version for it.\n",
        "# It costs us a lot to use a tokenizer so many countries are struggling for it.\n",
        "# !pip install indic-nlp-library\n",
        "from indicnlp.tokenize import indic_tokenize\n",
        "\n",
        "text = '\u092e\u0948\u0902 \u0906\u091c \u0916\u0941\u0936 \u0939\u0942\u0901'\n",
        "tokens = indic_tokenize.trivial_tokenize(text)\n",
        "print(tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STOP WORD REMOVAL\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# THE , IS , I , AM --> These are basically words which mean soo much but only used for a structured sentence.\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# METHOD A\n",
        "text = \"Ugh... The deliveries were DELAYED! I seriously hate waiting... #annoyed\"\n",
        "text_lower = text.lower()\n",
        "translator = str.maketrans('','',string.punctuation)\n",
        "clean_text = text_lower.translate(translator)\n",
        "tokens = clean_text.split()\n",
        "\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "print(\"Input: \",tokens)\n",
        "print(\"Result: \",filtered_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# STEMMING VS. LEMMATIZATION.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Stemming -> used to chop words to convert into root form but it might result in non-words\n",
        "# Lemmatization -> searches for a dictionary root form and add back meaning to our word.\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "words = [\"deliveries\",\"waiting\",\"delayed\",\"studies\"]\n",
        "\n",
        "for w in words:\n",
        "  print(w,\"|\",stemmer.stem(w),\"|\",lemmatizer.lemmatize(w))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VECTORIZATION ( TEXT TO NUMBERS )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This way of numbering text introduces bias -> so to prevent label encoding we use one-hot encoding but soon\n",
        "# if we realise in one-hot encoding it creates multiple columns which takes up lots of space.\n",
        "\n",
        "# HERE IT COMES -> BAG OF WORDS\n",
        "# All the sentences having similar words carry similar meaning\n",
        "# BIG PROBLEMS OF BAG OF WORDS\n",
        "# 1. FOR THIS SYSTEM : \"DOG BITES MAN\" WILL BE SAME AS \"MAN BITES DOG\" but in reality it is different."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# VECTOR EMBEDDINGS:\n",
        "# !pip install sentence_transformers\n",
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 2. Give it words\n",
        "words = [\"She is courageous\",'She acts strong']\n",
        "embeddings = model.encode(words)\n",
        "\n",
        "# 3. see the vectors\n",
        "for i in range(2):\n",
        "  print(\"Word: \",words[i])\n",
        "  print(\"Vector: \",embeddings[i][:3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COS_SIM IS USED TO TELL HOW SIMILAR OUR WORDS ARE.\n",
        "from sentence_transformers import util\n",
        "util.cos_sim(embeddings[0],embeddings[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# change filename if needed\n",
        "nb_file = \"NLP.ipynb\"\n",
        "\n",
        "with open(nb_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    nb = json.load(f)\n",
        "\n",
        "# REMOVE widgets metadata if present\n",
        "nb[\"metadata\"].pop(\"widgets\", None)\n",
        "\n",
        "# ALSO remove widget outputs inside cells\n",
        "for cell in nb.get(\"cells\", []):\n",
        "    cell.pop(\"metadata\", None)\n",
        "    if \"outputs\" in cell:\n",
        "        cell[\"outputs\"] = []\n",
        "\n",
        "with open(\"NLP_CLEAN.ipynb\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(nb, f, indent=2)\n",
        "\n",
        "print(\"\u2705 Clean notebook saved as NLP_CLEAN.ipynb\")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "execution_count": 57,
      "outputs": []
    }
  ]
}